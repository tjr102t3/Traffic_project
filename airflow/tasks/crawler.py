import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta
from io import StringIO
import re
from airflow.decorators import task, dag
from google.cloud import bigquery
from pandas_gbq import to_gbq
import pendulum

# === Airflow DAG è¨­å®š ===
@dag(
    start_date=pendulum.datetime(2025, 8, 1, tz="Asia/Taipei"),
    schedule_interval="0 */1 * * *",  # æ¯å°æ™‚åŸ·è¡Œä¸€æ¬¡
    catchup=False,
    tags=["freeway_traffic"]
)
def freeway_traffic_dag():

    # === åƒæ•¸è¨­å®š ===
    BASE_URL = "https://tisvcloud.freeway.gov.tw/history/TDCS/M05A/"
    COLUMNS = ["TimeStamp", "GantryFrom", "GantryTo", "VehicleType", "Speed", "Volume"]
    TARGET_IDS = {"05F0287N", "05F0055N", "05F0001N"}

    # === BigQuery è¨­å®š ===
    PROJECT_ID = "test123-467809"  # <--- è«‹æ›¿æ›æˆæ‚¨çš„ GCP å°ˆæ¡ˆ ID
    DATASET_ID = "bigquery"      # <--- è«‹æ›¿æ›æˆæ‚¨çš„ Dataset ID
    TABLE_ID = "traffic-data"          # <--- è«‹æ›¿æ›æˆæ‚¨åœ¨ BigQuery å»ºç«‹çš„ Table ID

    # === å·¥å…·: æŠ“å– HTML é€£çµ ===
    def get_links_by_suffix(url, suffix):
        try:
            res = requests.get(url)
            if res.status_code != 200:
                print(f"âŒ é€£ç·šå¤±æ•—: {url} ({res.status_code})")
                return []
            soup = BeautifulSoup(res.text, "html.parser")
            return [
                urljoin(url, a["href"])
                for td in soup.find_all("td", class_="indexcolname")
                for a in td.find_all("a")
                if a["href"].endswith(suffix)
            ]
        except Exception as e:
            print(f"âš ï¸ æŠ“å–é€£çµéŒ¯èª¤: {e}")
            return []

    # === ğŸ” å°‹æ‰¾æœ€è¿‘ 7 å¤©å…§çš„é€±å…­/é€±æ—¥çš„è³‡æ–™å¤¾ ===
    def find_latest_weekend_folder():
        today = datetime.now()
        for i in range(7):
            target_date = today - timedelta(days=i)
            if target_date.weekday() in [5, 6]:
                date_str = target_date.strftime("%Y%m%d")
                test_url = urljoin(BASE_URL, date_str + "/")
                print(f"ğŸ” å˜—è©¦å°‹æ‰¾é€±æœ«è³‡æ–™ï¼š{date_str}")
                if get_links_by_suffix(test_url, "/"):
                    return date_str
        return None

    # === æ‰¾æœ€æ–°çš„CSVæª”æ¡ˆä¸¦è§£ææ—¥æœŸ ===
    def get_latest_csv_link():
        date_str = find_latest_weekend_folder()
        if not date_str:
            print("âŒ æ‰¾ä¸åˆ°è¿‘ 7 æ—¥çš„é€±æœ«è³‡æ–™")
            return None, None

        date_obj = datetime.strptime(date_str, "%Y%m%d")

        if date_obj.weekday() == 6:
            target_date = date_obj + timedelta(days=1)
            target_hour = "00"
            print(f"âœ… æ‰¾åˆ°æ˜ŸæœŸæ—¥ï¼Œæº–å‚™å°‹æ‰¾ç¦®æ‹œä¸€ {target_hour} é»çš„è³‡æ–™")
        else:
            target_date = date_obj
            target_hour = "23"
            print(f"âœ… æ‰¾åˆ°æ˜ŸæœŸå…­ï¼Œæº–å‚™å°‹æ‰¾ç•¶æ—¥ {target_hour} é»çš„è³‡æ–™")

        target_date_str = target_date.strftime("%Y%m%d")
        target_url = urljoin(BASE_URL, target_date_str + "/")
        target_hour_url = urljoin(target_url, target_hour + "/")

        csv_links = get_links_by_suffix(target_hour_url, ".csv")

        if not csv_links:
            print(f"âš ï¸ æ‰¾ä¸åˆ° {target_date_str} {target_hour} é»çš„CSVæª”")
            return None, None
        
        sorted_links = sorted(csv_links)

        if target_hour == "00":
            latest_csv_url = sorted_links[0]
        else:
            latest_csv_url = sorted_links[-1]

        parsed = urlparse(latest_csv_url)
        filename = os.path.basename(parsed.path)

        match = re.search(r'(\d{8}_\d{6})', filename)
        if match:
            timestamp_str = match.group(1)
            return latest_csv_url, timestamp_str + ".csv"
        
        print("âš ï¸ ç„¡æ³•å¾æª”åä¸­è§£ææ—¥æœŸèˆ‡æ™‚é–“")
        return None, None

    # === Airflow ä»»å‹™: çˆ¬å–èˆ‡éæ¿¾è³‡æ–™ ===
    @task
    def scrape_and_filter_data():
        csv_url, filename = get_latest_csv_link()
        if not csv_url:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æœ€æ–°çš„ CSV æª”æ¡ˆ")
            return None, None

        try:
            print(f"â¬‡ï¸ ä¸‹è¼‰æœ€æ–°æª”æ¡ˆï¼š{csv_url}")
            r = requests.get(csv_url, timeout=20)
            if r.status_code != 200:
                print(f"âŒ ç„¡æ³•ä¸‹è¼‰: {csv_url}")
                return None, None
            df = pd.read_csv(StringIO(r.text), header=None)
            if df.shape[1] != 6:
                print("âš ï¸ æ¬„ä½æ•¸é‡éŒ¯èª¤")
                return None, None
            df.columns = COLUMNS
            df = df[(df["GantryFrom"].isin(TARGET_IDS)) & (df["Speed"] != 0)]
            
            # âœ… æ–°å¢ 'Date' å’Œ 'Time' æ¬„ä½
            df['Date'] = pd.to_datetime(df['TimeStamp']).dt.strftime('%Y/%m/%d')
            df['Time'] = pd.to_datetime(df['TimeStamp']).dt.strftime('%H:%M')

            if df.empty:
                print("âš ï¸ ç„¡å¯ç”¨è³‡æ–™é€²è¡Œçµ±è¨ˆ")
                return None, None

            timestamp = df["TimeStamp"].iloc[0]
            grouped = df.groupby(["GantryFrom", "GantryTo"]).agg({
                "Speed": "mean",
                "Volume": "sum"
            }).reset_index()

            grouped = grouped[grouped["GantryTo"].isin(TARGET_IDS)]
            grouped.insert(0, "Time", df['Time'].iloc[0])
            grouped.insert(0, "Date", df['Date'].iloc[0])
            grouped.insert(2, "TimeStamp", timestamp)

            return grouped

        except Exception as e:
            print(f"âš ï¸ ä¸‹è¼‰å¤±æ•—ï¼š{e}")
            return None

    # === Airflow ä»»å‹™: å°‡è³‡æ–™å¯«å…¥ BigQuery ===
    @task
    def load_to_bigquery(df):
        if df is None:
            print("âš ï¸ ç„¡è³‡æ–™å¯ä¸Šå‚³è‡³ BigQuery")
            return

        # é€™è£¡çš„ 'if_exists' åƒæ•¸éå¸¸é‡è¦ï¼Œè«‹æ ¹æ“šæ‚¨çš„éœ€æ±‚é¸æ“‡ï¼š
        # 'append': å°‡æ–°çš„è³‡æ–™é™„åŠ åˆ°ç¾æœ‰è³‡æ–™è¡¨ã€‚
        # 'replace': åˆªé™¤ç¾æœ‰è³‡æ–™è¡¨ï¼Œç”¨æ–°çš„ DataFrame æ›¿æ›ã€‚
        # 'fail': å¦‚æœè³‡æ–™è¡¨å·²å­˜åœ¨ï¼Œå‰‡æ‹‹å‡ºéŒ¯èª¤ã€‚
        # é€™è£¡é¸æ“‡ 'append' ä»¥ç¢ºä¿æ¯æ¬¡çˆ¬èŸ²éƒ½èƒ½æ–°å¢è³‡æ–™
        try:
            to_gbq(
                dataframe=df,
                destination_table=f"{DATASET_ID}.{TABLE_ID}",
                project_id=PROJECT_ID,
                if_exists="append",
                chunksize=10000,
                table_schema=[
                    {'name': 'Date', 'type': 'DATE'},
                    {'name': 'Time', 'type': 'TIME'},
                    {'name': 'TimeStamp', 'type': 'DATETIME'},
                    {'name': 'GantryFrom', 'type': 'STRING'},
                    {'name': 'GantryTo', 'type': 'STRING'},
                    {'name': 'Speed', 'type': 'FLOAT'},
                    {'name': 'Volume', 'type': 'FLOAT'}
                ]
            )
            print(f"âœ… è³‡æ–™å·²æˆåŠŸå¯«å…¥ BigQuery: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}")

        except Exception as e:
            print(f"âŒ å¯«å…¥ BigQuery å¤±æ•—: {e}")
            raise # æ‹‹å‡ºéŒ¯èª¤è®“ Airflow çŸ¥é“ä»»å‹™å¤±æ•—

    # === å®šç¾©ä»»å‹™æµç¨‹ ===
    # é€™é‚Šç›´æ¥å°‡ä¸€å€‹ä»»å‹™çš„çµæœå‚³çµ¦ä¸‹ä¸€å€‹ä»»å‹™
    scraped_data = scrape_and_filter_data()
    load_to_bigquery(scraped_data)

# åŸ·è¡Œ DAG
freeway_traffic_dag()