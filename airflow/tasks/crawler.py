import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta
from io import StringIO
import re
from pandas_gbq import to_gbq
from google.cloud import bigquery

# === åƒæ•¸è¨­å®š ===
BASE_URL = "https://tisvcloud.freeway.gov.tw/history/TDCS/M05A/"
COLUMNS = ["TimeStamp", "GantryFrom", "GantryTo", "VehicleType", "Avg_speed", "Total_volume"]
TARGET_IDS = {"05F0287N", "05F0055N"}

# === BigQuery è¨­å®š ===
# ***éœ€æ›´æ”¹***
PROJECT_ID = "test123-467809"
# ***éœ€æ›´æ”¹***
DATASET_ID = "bigquery"
# === æ–°å¢è¡¨æ ¼åç¨±å°æ‡‰é—œä¿‚ ===
TABLE_MAPPING = {
    "05F0287N": "traffic-data-05F0287N",
    "05F0055N": "traffic-data-05F0055N"
}
TABLE_SCHEMA = [
    {'name': 'Date', 'type': 'DATE'},
    {'name': 'Time', 'type': 'TIME'},
    {'name': 'TimeStamp', 'type': 'DATETIME'},
    {'name': 'GantryFrom', 'type': 'STRING'},
    {'name': 'GantryTo', 'type': 'STRING'},
    {'name': 'Avg_speed', 'type': 'FLOAT'},
    {'name': 'Total_volume', 'type': 'FLOAT'}
]

# === å·¥å…·å‡½å¼ ===

def get_links_by_suffix(url, suffix):
    """å¾ç¶²é ä¸Šçˆ¬å–ç¬¦åˆç‰¹å®šå‰¯æª”åçš„é€£çµ"""
    try:
        res = requests.get(url, timeout=10)
        if res.status_code != 200:
            print(f"âŒ é€£ç·šå¤±æ•—: {url} ({res.status_code})")
            return []
        soup = BeautifulSoup(res.text, "html.parser")
        return [
            urljoin(url, a["href"])
            for td in soup.find_all("td", class_="indexcolname")
            for a in td.find_all("a")
            if a["href"].endswith(suffix)
        ]
    except Exception as e:
        print(f"âš ï¸ æŠ“å–é€£çµéŒ¯èª¤: {e}")
        return []

def check_data_exists(table_id, timestamp, gantry_from_id):
    """æª¢æŸ¥ BigQuery ä¸­æ˜¯å¦å·²å­˜åœ¨è©²ç­†è³‡æ–™ï¼Œé¿å…é‡è¤‡å¯«å…¥"""
    client = bigquery.Client(project=PROJECT_ID)
    query = f"""
    SELECT count(*) FROM `{PROJECT_ID}.{DATASET_ID}.{table_id}`
    WHERE TimeStamp = '{timestamp}' AND GantryFrom = '{gantry_from_id}'
    """
    query_job = client.query(query)
    results = query_job.result()
    for row in results:
        if row[0] > 0:
            return True
    return False

# === ä»»å‹™å‡½å¼ ===

def get_target_csv_info(**kwargs):
    """
    æ ¹æ“š DAG åŸ·è¡Œæ™‚é–“åˆ¤æ–·æ˜¯å¦çˆ¬å–ï¼Œä¸¦è¿”å›ç›®æ¨™ CSV æª”æ¡ˆçš„è³‡è¨Šã€‚
    """
    now = kwargs['logical_date']
    weekday = now.weekday() # æ˜ŸæœŸä¸€(0)åˆ°æ˜ŸæœŸæ—¥(6)

    perform_scrape = False
    
    if weekday == 5 or weekday == 6: # æ˜ŸæœŸå…­ã€æ—¥
        perform_scrape = True
    elif weekday == 0: # æ˜ŸæœŸä¸€
        if now.hour == 0 and now.minute == 0:
            perform_scrape = True
    
    if not perform_scrape:
        print(f"ğŸ” ç›®å‰é‚è¼¯æ—¥æœŸ {now.strftime('%Y-%m-%d %H:%M')} ä¸åœ¨æŒ‡å®šçˆ¬å–æ™‚æ®µï¼Œä»»å‹™å°‡è·³éã€‚")
        return None

    target_date_str = now.strftime("%Y%m%d")
    target_hour_str = now.strftime("%H")
    
    target_url_date = urljoin(BASE_URL, target_date_str + "/")
    target_url_hour = urljoin(target_url_date, target_hour_str + "/")

    csv_links = get_links_by_suffix(target_url_hour, ".csv")

    if not csv_links:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {target_date_str} {target_hour_str} é»çš„ CSV æª”")
        return None

    sorted_links = sorted(csv_links)
    
    target_filename_prefix = now.strftime('%Y%m%d_%H%M')
    for link in reversed(sorted_links):
        if target_filename_prefix in link:
            parsed = urlparse(link)
            filename = os.path.basename(parsed.path)
            match = re.search(r'(\d{8}_\d{6})', filename)
            if match:
                timestamp_str = match.group(1)
                return {'url': link, 'timestamp': timestamp_str}
    
    latest_csv_url = sorted_links[-1]
    parsed = urlparse(latest_csv_url)
    filename = os.path.basename(parsed.path)
    match = re.search(r'(\d{8}_\d{6})', filename)
    if match:
        timestamp_str = match.group(1)
        return {'url': latest_csv_url, 'timestamp': timestamp_str}

    print("âš ï¸ ç„¡æ³•å¾æª”åä¸­è§£ææ—¥æœŸèˆ‡æ™‚é–“")
    return None

def scrape_and_process_data(csv_info):
    """ä¸‹è¼‰ã€æ¸…æ´—ä¸¦è™•ç† CSV è³‡æ–™ï¼Œé‡å°æ¯å€‹é–€æ¶é€²è¡Œèšåˆ"""
    if not csv_info:
        print("ç„¡çˆ¬å–è³‡è¨Šï¼Œä»»å‹™çµæŸã€‚")
        return None

    csv_url = csv_info['url']
    timestamp_str = csv_info['timestamp']

    try:
        print(f"â¬‡ï¸ ä¸‹è¼‰æª”æ¡ˆï¼š{csv_url}")
        r = requests.get(csv_url, timeout=20)
        r.raise_for_status()
        
        df = pd.read_csv(StringIO(r.text), header=None, names=COLUMNS)
        
        filtered_df = df[
            (df["GantryFrom"].isin(TARGET_IDS)) & 
            (df["Avg_speed"] != 0)
        ].copy()

        if filtered_df.empty:
            print(f"âš ï¸ éæ¿¾å¾Œç„¡å¯ç”¨è³‡æ–™é€²è¡Œçµ±è¨ˆã€‚")
            return None
        
        filtered_df['TimeStamp'] = pd.to_datetime(filtered_df['TimeStamp'])
        filtered_df['Date'] = filtered_df['TimeStamp'].dt.date
        filtered_df['Time'] = filtered_df['TimeStamp'].dt.time
        
        grouped_df = filtered_df.groupby(["GantryFrom", "GantryTo"]).agg(
            Avg_speed_mean=("Avg_speed", "mean"),
            Total_volume_sum=("Total_volume", "sum")
        ).reset_index()

        grouped_df.insert(0, "Date", filtered_df['Date'].iloc[0])
        grouped_df.insert(1, "Time", filtered_df['Time'].iloc[0])
        grouped_df.insert(2, "TimeStamp", filtered_df['TimeStamp'].iloc[0])
        
        grouped_df = grouped_df.rename(columns={
            'Avg_speed_mean': 'Avg_speed',
            'Total_volume_sum': 'Total_volume'
        })
        
        result = {}
        for gantry_id in TARGET_IDS:
            df_gantry = grouped_df[grouped_df["GantryFrom"] == gantry_id].copy()
            if not df_gantry.empty:
                result[gantry_id] = df_gantry
        
        return result

    except requests.exceptions.RequestException as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—ï¼ŒHTTP éŒ¯èª¤ï¼š{e}")
        return None
    except Exception as e:
        print(f"âš ï¸ è³‡æ–™è™•ç†å¤±æ•—ï¼š{e}")
        return None

def load_to_bigquery(processed_data):
    """å°‡è™•ç†å¾Œçš„è³‡æ–™åˆ†åˆ¥å¯«å…¥ BigQuery"""
    if not processed_data:
        print("ç„¡è³‡æ–™å¯ä¸Šå‚³è‡³ BigQueryã€‚")
        return

    client = bigquery.Client(project=PROJECT_ID)

    for gantry_id, df in processed_data.items():
        table_id = TABLE_MAPPING.get(gantry_id)
        if not table_id:
            print(f"âŒ æ‰¾ä¸åˆ° {gantry_id} å°æ‡‰çš„è¡¨æ ¼åç¨±ï¼Œè·³éå¯«å…¥ã€‚")
            continue
            
        df = df[['Date', 'Time', 'TimeStamp', 'GantryFrom', 'GantryTo', 'Avg_speed', 'Total_volume']]
        
        timestamp = df['TimeStamp'].iloc[0]
        if check_data_exists(table_id, timestamp, gantry_id):
            print(f"âš ï¸ {timestamp} çš„ {gantry_id} è³‡æ–™å·²å­˜åœ¨æ–¼è¡¨æ ¼ '{table_id}'ï¼Œè·³éå¯«å…¥ã€‚")
            continue

        try:
            to_gbq(
                dataframe=df,
                destination_table=f"{DATASET_ID}.{table_id}",
                project_id=PROJECT_ID,
                if_exists="append",
                chunksize=10000,
                table_schema=TABLE_SCHEMA
            )
            formatted_timestamp = timestamp.strftime('%Y%m%d_%H%M%S')
            print(f"âœ… è³‡æ–™å·²æˆåŠŸå¯«å…¥ BigQueryï¼Œé–€æ¶ {gantry_id}ï¼Œè¡¨æ ¼ '{table_id}'ã€‚")
            print(f"è¨˜éŒ„åç¨±ç¯„ä¾‹ï¼š{formatted_timestamp}_{gantry_id}")

        except Exception as e:
            print(f"âŒ å¯«å…¥ BigQuery å¤±æ•—ï¼Œé–€æ¶ {gantry_id}ï¼Œè¡¨æ ¼ '{table_id}': {e}")
            raise