import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta
from io import StringIO
import re
from airflow.decorators import task
from pandas_gbq import to_gbq

# === åƒæ•¸è¨­å®š ===
BASE_URL = "https://tisvcloud.freeway.gov.tw/history/TDCS/M05A/"
COLUMNS = ["TimeStamp", "GantryFrom", "GantryTo", "VehicleType", "Speed", "Volume"]
TARGET_IDS = {"05F0287N", "05F0055N", "05F0001N"}

# === BigQuery è¨­å®š ===
PROJECT_ID = "test123-467809"
DATASET_ID = "bigquery"
TABLE_ID = "traffic-data"

# === å·¥å…·å‡½å¼ ===
def get_links_by_suffix(url, suffix):
    try:
        res = requests.get(url)
        if res.status_code != 200:
            print(f"âŒ é€£ç·šå¤±æ•—: {url} ({res.status_code})")
            return []
        soup = BeautifulSoup(res.text, "html.parser")
        return [
            urljoin(url, a["href"])
            for td in soup.find_all("td", class_="indexcolname")
            for a in td.find_all("a")
            if a["href"].endswith(suffix)
        ]
    except Exception as e:
        print(f"âš ï¸ æŠ“å–é€£çµéŒ¯èª¤: {e}")
        return []

def find_latest_weekend_folder():
    today = datetime.now()
    for i in range(7):
        target_date = today - timedelta(days=i)
        if target_date.weekday() in [5, 6]:
            date_str = target_date.strftime("%Y%m%d")
            test_url = urljoin(BASE_URL, date_str + "/")
            print(f"ğŸ” å˜—è©¦å°‹æ‰¾é€±æœ«è³‡æ–™ï¼š{date_str}")
            if get_links_by_suffix(test_url, "/"):
                return date_str
    return None

def get_latest_csv_link():
    date_str = find_latest_weekend_folder()
    if not date_str:
        print("âŒ æ‰¾ä¸åˆ°è¿‘ 7 æ—¥çš„é€±æœ«è³‡æ–™")
        return None, None

    date_obj = datetime.strptime(date_str, "%Y%m%d")
    
    if date_obj.weekday() == 6:
        target_date = date_obj + timedelta(days=1)
        target_hour = "00"
        print(f"âœ… æ‰¾åˆ°æ˜ŸæœŸæ—¥ï¼Œæº–å‚™å°‹æ‰¾ç¦®æ‹œä¸€ {target_hour} é»çš„è³‡æ–™")
    else:
        target_date = date_obj
        target_hour = "23"
        print(f"âœ… æ‰¾åˆ°æ˜ŸæœŸå…­ï¼Œæº–å‚™å°‹æ‰¾ç•¶æ—¥ {target_hour} é»çš„è³‡æ–™")

    target_date_str = target_date.strftime("%Y%m%d")
    target_url = urljoin(BASE_URL, target_date_str + "/")
    target_hour_url = urljoin(target_url, target_hour + "/")

    csv_links = get_links_by_suffix(target_hour_url, ".csv")

    if not csv_links:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {target_date_str} {target_hour} é»çš„CSVæª”")
        return None, None
    
    sorted_links = sorted(csv_links)

    if target_hour == "00":
        latest_csv_url = sorted_links[0]
    else:
        latest_csv_url = sorted_links[-1]

    parsed = urlparse(latest_csv_url)
    filename = os.path.basename(parsed.path)

    match = re.search(r'(\d{8}_\d{6})', filename)
    if match:
        timestamp_str = match.group(1)
        return latest_csv_url, timestamp_str + ".csv"
    
    print("âš ï¸ ç„¡æ³•å¾æª”åä¸­è§£ææ—¥æœŸèˆ‡æ™‚é–“")
    return None, None

# === Airflow ä»»å‹™: çˆ¬å–èˆ‡éæ¿¾è³‡æ–™ ===
@task
def scrape_and_filter_data():
    csv_url, filename = get_latest_csv_link()
    if not csv_url:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æœ€æ–°çš„ CSV æª”æ¡ˆ")
        return None

    try:
        print(f"â¬‡ï¸ ä¸‹è¼‰æœ€æ–°æª”æ¡ˆï¼š{csv_url}")
        r = requests.get(csv_url, timeout=20)
        if r.status_code != 200:
            print(f"âŒ ç„¡æ³•ä¸‹è¼‰: {csv_url}")
            return None
        df = pd.read_csv(StringIO(r.text), header=None)
        if df.shape[1] != 6:
            print("âš ï¸ æ¬„ä½æ•¸é‡éŒ¯èª¤")
            return None
        df.columns = COLUMNS
        df = df[(df["GantryFrom"].isin(TARGET_IDS)) & (df["Speed"] != 0)]
        
        df['Date'] = pd.to_datetime(df['TimeStamp']).dt.date
        df['Time'] = pd.to_datetime(df['TimeStamp']).dt.time

        if df.empty:
            print("âš ï¸ ç„¡å¯ç”¨è³‡æ–™é€²è¡Œçµ±è¨ˆ")
            return None

        timestamp = df["TimeStamp"].iloc[0]
        grouped = df.groupby(["GantryFrom", "GantryTo"]).agg({
            "Speed": "mean",
            "Volume": "sum"
        }).reset_index()

        grouped = grouped[grouped["GantryTo"].isin(TARGET_IDS)]
        grouped.insert(0, "Time", df['Time'].iloc[0])
        grouped.insert(0, "Date", df['Date'].iloc[0])
        grouped.insert(2, "TimeStamp", timestamp)
        
        # === å°‡é€™æ®µç¨‹å¼ç¢¼è²¼åœ¨é€™è£¡ï¼ ===
        # å°‡ TimeStamp æ¬„ä½æ˜ç¢ºåœ°è½‰æ›ç‚º datetime ç‰©ä»¶
        # é€™èƒ½ç¢ºä¿è³‡æ–™å‹æ…‹èˆ‡ BigQuery Schema ä¸­çš„ DATETIME å»åˆ
        grouped['TimeStamp'] = pd.to_datetime(grouped['TimeStamp'])
        
        # ç¢ºä¿æ¬„ä½é †åºæ­£ç¢º (å¦‚æœæ‚¨æœ‰ä¿ç•™ table_schema)
        # é€™æ˜¯ç‚ºäº†é¿å…æ¬„ä½é †åºä¸ç¬¦å°è‡´å¯«å…¥å¤±æ•—
        grouped = grouped[[
            'Date', 
            'Time', 
            'TimeStamp', 
            'GantryFrom', 
            'GantryTo', 
            'Speed', 
            'Volume'
        ]]

        return grouped

    except Exception as e:
        print(f"âš ï¸ ä¸‹è¼‰å¤±æ•—ï¼š{e}")
        return None

# === Airflow ä»»å‹™: å°‡è³‡æ–™å¯«å…¥ BigQuery ===
@task
def load_to_bigquery(df):
    if df is None:
        print("âš ï¸ ç„¡è³‡æ–™å¯ä¸Šå‚³è‡³ BigQuery")
        return

    try:
        to_gbq(
            dataframe=df,
            destination_table=f"{DATASET_ID}.{TABLE_ID}",
            project_id=PROJECT_ID,
            if_exists="append",
            chunksize=10000,
            table_schema=[
                {'name': 'Date', 'type': 'DATE'},
                {'name': 'Time', 'type': 'TIME'},
                {'name': 'TimeStamp', 'type': 'DATETIME'},
                {'name': 'GantryFrom', 'type': 'STRING'},
                {'name': 'GantryTo', 'type': 'STRING'},
                {'name': 'Speed', 'type': 'FLOAT'},
                {'name': 'Volume', 'type': 'FLOAT'}
            ]
        )
        print(f"âœ… è³‡æ–™å·²æˆåŠŸå¯«å…¥ BigQuery: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}")

    except Exception as e:
        print(f"âŒ å¯«å…¥ BigQuery å¤±æ•—: {e}")
        raise