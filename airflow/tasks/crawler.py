import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta
from io import StringIO
import re
from airflow.decorators import task

# === åƒæ•¸è¨­å®š ===
BASE_URL = "https://tisvcloud.freeway.gov.tw/history/TDCS/M05A/"
COLUMNS = ["TimeStamp", "GantryFrom", "GantryTo", "VehicleType", "Speed", "Volume"]
TARGET_IDS = {"05F0287N", "05F0055N", "05F0001N"}

# === å·¥å…·: æŠ“å– HTML é€£çµ ===
def get_links_by_suffix(url, suffix):
    try:
        res = requests.get(url)
        if res.status_code != 200:
            print(f"âŒ é€£ç·šå¤±æ•—: {url} ({res.status_code})")
            return []
        soup = BeautifulSoup(res.text, "html.parser")
        return [
            urljoin(url, a["href"])
            for td in soup.find_all("td", class_="indexcolname")
            for a in td.find_all("a")
            if a["href"].endswith(suffix)
        ]
    except Exception as e:
        print(f"âš ï¸ æŠ“å–é€£çµéŒ¯èª¤: {e}")
        return []

# === ğŸ” å°‹æ‰¾æœ€è¿‘ 7 å¤©å…§çš„é€±å…­/é€±æ—¥çš„è³‡æ–™å¤¾ ===
def find_latest_weekend_folder():
    today = datetime.now()
    for i in range(7):
        target_date = today - timedelta(days=i)
        if target_date.weekday() in [5, 6]:
            date_str = target_date.strftime("%Y%m%d")
            test_url = urljoin(BASE_URL, date_str + "/")
            print(f"ğŸ” å˜—è©¦å°‹æ‰¾é€±æœ«è³‡æ–™ï¼š{date_str}")
            if get_links_by_suffix(test_url, "/"):
                return date_str
    return None

# === æ‰¾æœ€æ–°çš„CSVæª”æ¡ˆä¸¦è§£ææ—¥æœŸ ===
def get_latest_csv_link():
    date_str = find_latest_weekend_folder()
    if not date_str:
        print("âŒ æ‰¾ä¸åˆ°è¿‘ 7 æ—¥çš„é€±æœ«è³‡æ–™")
        return None, None

    date_obj = datetime.strptime(date_str, "%Y%m%d")

    if date_obj.weekday() == 6:
        target_date = date_obj + timedelta(days=1)
        target_hour = "00"
        print(f"âœ… æ‰¾åˆ°æ˜ŸæœŸæ—¥ï¼Œæº–å‚™å°‹æ‰¾ç¦®æ‹œä¸€ {target_hour} é»çš„è³‡æ–™")
    else:
        target_date = date_obj
        target_hour = "23"
        print(f"âœ… æ‰¾åˆ°æ˜ŸæœŸå…­ï¼Œæº–å‚™å°‹æ‰¾ç•¶æ—¥ {target_hour} é»çš„è³‡æ–™")

    target_date_str = target_date.strftime("%Y%m%d")
    target_url = urljoin(BASE_URL, target_date_str + "/")
    target_hour_url = urljoin(target_url, target_hour + "/")

    csv_links = get_links_by_suffix(target_hour_url, ".csv")

    if not csv_links:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {target_date_str} {target_hour} é»çš„CSVæª”")
        return None, None
    
    sorted_links = sorted(csv_links)

    if target_hour == "00":
        latest_csv_url = sorted_links[0]
    else:
        latest_csv_url = sorted_links[-1]

    parsed = urlparse(latest_csv_url)
    filename = os.path.basename(parsed.path)

    match = re.search(r'(\d{8}_\d{6})', filename)
    if match:
        timestamp_str = match.group(1)
        return latest_csv_url, timestamp_str + ".csv"
    
    print("âš ï¸ ç„¡æ³•å¾æª”åä¸­è§£ææ—¥æœŸèˆ‡æ™‚é–“")
    return None, None

# === Airflow ä»»å‹™: çˆ¬å–èˆ‡éæ¿¾è³‡æ–™ ===
@task
def scrape_and_filter_data():
    csv_url, filename = get_latest_csv_link()
    if not csv_url:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æœ€æ–°çš„ CSV æª”æ¡ˆ")
        return None, None

    try:
        print(f"â¬‡ï¸ ä¸‹è¼‰æœ€æ–°æª”æ¡ˆï¼š{csv_url}")
        r = requests.get(csv_url, timeout=20)
        if r.status_code != 200:
            print(f"âŒ ç„¡æ³•ä¸‹è¼‰: {csv_url}")
            return None, None
        df = pd.read_csv(StringIO(r.text), header=None)
        if df.shape[1] != 6:
            print("âš ï¸ æ¬„ä½æ•¸é‡éŒ¯èª¤")
            return None, None
        df.columns = COLUMNS
        df = df[(df["GantryFrom"].isin(TARGET_IDS)) & (df["Speed"] != 0)]
        
        # âœ… æ–°å¢ 'Date' å’Œ 'Time' æ¬„ä½
        df['Date'] = pd.to_datetime(df['TimeStamp']).dt.strftime('%Y/%m/%d')
        df['Time'] = pd.to_datetime(df['TimeStamp']).dt.strftime('%H:%M')

        if df.empty:
            print("âš ï¸ ç„¡å¯ç”¨è³‡æ–™é€²è¡Œçµ±è¨ˆ")
            return None, None

        timestamp = df["TimeStamp"].iloc[0]
        grouped = df.groupby(["GantryFrom", "GantryTo"]).agg({
            "Speed": "mean",
            "Volume": "sum"
        }).reset_index()

        grouped = grouped[grouped["GantryTo"].isin(TARGET_IDS)]
        grouped.insert(0, "Time", df['Time'].iloc[0])
        grouped.insert(0, "Date", df['Date'].iloc[0])
        grouped.insert(2, "TimeStamp", timestamp)

        return grouped, filename
    except Exception as e:
        print(f"âš ï¸ ä¸‹è¼‰å¤±æ•—ï¼š{e}")
        return None, None

# === Airflow ä»»å‹™: å„²å­˜è³‡æ–™ï¼ˆæ­¤è™•ç‚ºç¯„ä¾‹ï¼Œå¯æ›¿æ›æˆBigQueryä¸Šå‚³ï¼‰ ===
@task
def save_data(data_tuple):
    df, filename = data_tuple
    if df is None or filename is None:
        print("âš ï¸ ç„¡è³‡æ–™å¯å„²å­˜")
        return

    output_path = os.path.join(os.getcwd(), filename)
    df.to_csv(output_path, index=False, encoding="utf-8-sig")

    print("ğŸ“Š åˆ†çµ„çµ±è¨ˆçµæœï¼ˆSpeed å¹³å‡, Volume åŠ ç¸½ï¼‰ï¼š")
    print(df)
    print(f"âœ… çµ±è¨ˆçµæœå·²å„²å­˜ï¼š{output_path}")

    # é€™è£¡å¯ä»¥æ ¹æ“šéœ€æ±‚ï¼Œæ›¿æ›æˆä¸Šå‚³è‡³BigQueryçš„é‚è¼¯